"""Evaluation functions for point alignment methods
"""
import argparse
import pickle as pk
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import wandb
from src.ri_distances.icp.icp import IcpPredictor
from src.ri_distances.pnt_cloud_generation import (generate_permutation_matrix,
                                                   generate_rotation_matrix,
                                                   get_gaussian_point_cloud,
                                                   get_spiral)
from src.ri_distances.rotation_predictor import WS
from src.ri_distances.SGW.risgw import RisgwPredictor
from src.ri_distances.SGW.sgw_pytorch import sgw_gpu_np
from src.se3.visualization import viz_point_cloud
from tqdm import tqdm, trange

plt.style.use('ggplot')

torch.set_default_dtype(torch.float32)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


@dataclass(order=True)
class DataParam():
    """
    Parameter for the data obtention

    Args:
        metric_name (str): either 'SGW' or 'WS'
        data_func (function): function to generate the source point cloud
        metric_func (function): function to compute a metric between two np.array point clouds
        N_pts (int): numnber of points in the point cloud
        permuet (bool): wheter to permute the rows of the point cloud across source and target
        noise_factor (float): absolute noise factor
    """
    data_func: 'function'
    metric_func: 'function' = sgw_gpu_np
    N_pts: int = 50
    permute: bool = True
    theta: float = None
    noise_factor: float = 0.0
    metric_name: str = None

    def __post_init__(self):
        if self.metric_name is None:
            self.metric_name = self.metric_func.__name__

    def __hash__(self):
        values = tuple(self.__dict__.values())
        return hash(values)

    def generate_target(self):
        """
        Generate the target point cloud given the source point cloud and the
        current parameters

        Args:
            src (np.array): source point cloud

        Returns:
            [type]: [description]
        """
        # Source point generation
        src_pnt_cloud = self.data_func(N_pts=self.N_pts)

        Q = generate_rotation_matrix(theta=self.theta)
        if self.permute:
            P = generate_permutation_matrix(self.N_pts)
        else:
            P = np.eye(self.N_pts)

        # Rotation
        trgt_pnt_cloud = src_pnt_cloud @ Q
        # Permutation
        trgt_pnt_cloud = P @ trgt_pnt_cloud
        # Noise addition
        trgt_pnt_cloud += np.random.randn(*
                                          trgt_pnt_cloud.shape) * self.noise_factor

        # We retrieve the clean target for comparison
        clean_target_pnt_cloud = P @ src_pnt_cloud @ Q

        return src_pnt_cloud, clean_target_pnt_cloud, trgt_pnt_cloud


def evaluate_data_param(p, predictors, use_wandb=False):
    """ Evaluate the predictors passed in parameter on the data generated by data_func
    using data_func_args. We aim at aligning a clean point cloud (src) with a potentially noisy,
    potentially rotated, potentially permutated version of it (target). The denoised version of the
    target is referred to as the unnoised target. The prediction of the algorithm to align src on target
    is referred to as pred
    evaluate_predictors computes the difference in WS(pred,target) and WS(unnoised target,target) along
    with the computational time taken: ideally both of these metrics should tend to zero.

    Args:
        data_func (func): function to generate data
        data_func_args (tuple): positional parameters to pass to data_func
        predictors (RotationPredictor): predictor having a predict method
        permute (bool): whether to randomly permute the data
        noise_factor (float): amplitude of the noise
        use_wandb (bool, optional): whether to record experiment on wandb.

    Returns:
        [list of dict]: list of dict with 'time' 'metric' 'fig' and 'name' entries
    """

    # target point cloud is the point cloud we aim to obtain
    src_pnt_cloud, clean_trgt, trgt_pnt_cloud = p.generate_target()

    if p.noise_factor == 0:
        assert(np.isclose(trgt_pnt_cloud, clean_trgt).all())

    results = []
    for predictor in tqdm(predictors, desc='Iterating predictors', leave=False):
        result = execute_run(predictor, p, src_pnt_cloud, trgt_pnt_cloud,
                             clean_trgt, use_wandb=use_wandb)
        results.append(result)

    return results


def execute_run(predictor, p, src_pnt_cloud, trgt_pnt_cloud, no_noise_target, use_wandb=False):
    """
    Execute one run with the given parameters, in particular with the combination of the given
    predictor and data parameter p.

    Args:
        predictor ([type]): Predictor: a RotationPredictor
        p (DataParam): data parameter
        src_pnt_cloud (np.array): source point cloud
        trgt_pnt_cloud (np.array): target point cloud
        no_noise_target (np.array): ideal point cloud to retrieve
        use_wandb (bool, optional): whether to log the metrics on wandb. Defaults to False.

    Returns:
        result (dict): the various parameters and metrics related to one run of the combination
        of a data param and a predictor
    """
    # Infer the rotation
    start_time = time.time()
    pred_pnt_cloud = predictor.predict(src_pnt_cloud, trgt_pnt_cloud)
    end_time = time.time()
    duration = end_time - start_time
    fig = viz_point_cloud([(no_noise_target, 'clean_trgt'),
                           (pred_pnt_cloud, 'pred')])
    fig.suptitle(str(p))
    # Best metric we can reach since we only rotate the input (reference)
    min_metric = WS(no_noise_target, trgt_pnt_cloud)

    # Actual metric
    metric = WS(pred_pnt_cloud, trgt_pnt_cloud)

    result = {'data_param': p,
              'predictor': predictor,
              'N_pts': p.N_pts,
              'time': duration,
              'metric': metric - min_metric,
              'name': str(predictor),
              'metric_name': p.metric_name,
              'fig': fig}
    if use_wandb:
        run = wandb.init(project='point alignment', reinit=True)

        # update wandb with data creation parameters
        wandb.config.update({'data_func': p.data_func.__name__,
                             'permute': p.permute,
                             'noise_factor': p.noise_factor})

        # argument for data function
        wandb.config.update(p.data_func_args)

        # update wandb with predictor parameters
        wandb.config.update(predictor.__dict__)
        wandb.log({"point cloud": wandb.Image(fig)})
        wandb.log({"loss": metric})
        run.finish()
    return result


def plot_crossed_boxplot(x_data, y_data, label, ax):
    """
    Plot a 2d boxplot on the provided axis

    Args:
        x_data (np.array): x values
        y_data (np.array): y values
        label (string): label for the boxplot
        ax (plt.axis): ax to plot on
    """
    x_pos = np.median(x_data)
    x25 = np.quantile(x_data, 0.25)
    x75 = np.quantile(x_data, 0.75)
    y_pos = np.median(y_data)
    y25 = np.quantile(y_data, 0.25)
    y75 = np.quantile(y_data, 0.75)
    color = np.random.rand(3,)
    ewidth = 3
    ax.errorbar(x_pos, y_pos, yerr=[
                [abs(y_pos-y25)], [abs(y_pos-y75)]], capsize=5.0, color=color, label=label, elinewidth=ewidth)
    ax.errorbar(x_pos, y_pos, xerr=[
                [abs(x_pos-x25)], [abs(x_pos-x75)]], capsize=5.0, color=color, elinewidth=ewidth)
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])

    # Put a legend to the right of the current axis
    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 10})


def display_metric_vs_time_results(results, fig, ax):
    """
    Display the metrics as generated by evaluate_predictor as boxplots

    Args:
        results (list of dict): as returned by evaluate_predictor
    """
    results_df = pd.DataFrame(results)
    data = results_df.groupby('name').agg(list).to_dict(orient='index')
    N_runs = results_df.groupby('name').agg(len).iloc[0, 0]
    p = results_df.iloc[0]['data_param']
    fig.suptitle(
        f"Datafunc:{p.data_func.__name__},Metric={p.metric_func},N_runs={N_runs},N_pts={p.N_pts},permute={p.permute},noise={p.noise_factor}",
        fontsize=15)

    ax.set_xlabel('Time [sec]', fontsize=20)
    ax.set_ylabel(p.metric_name, fontsize=20)
    for model_name, model_data in data.items():
        plot_crossed_boxplot(
            x_data=model_data['time'], y_data=model_data['metric'], label=model_name, ax=ax)
    plt.close()


def load_most_recent(res_dir):
    max_date = None
    for path in res_dir.glob("*.pk"):
        try:
            curr_date = datetime.strptime(
                path.stem, "%m_%d_%Y_%H_%M_%S")
            if max_date is None or curr_date > max_date:
                max_date = curr_date
                max_path = path
        except:
            continue
    return pk.load(max_path.open('rb'))


def main():

    run_name = time.strftime("%m_%d_%Y_%H_%M_%S")
    run_path = Path("models").joinpath(run_name).with_suffix('.pk')

    parser = argparse.ArgumentParser(description='Experiment argument parsing')
    parser.add_argument(
        '-d', '--dataset', help='dataset type(spiral="s",gaussian="g")', default='s')
    parser.add_argument(
        '-m', '--metric_func', help='metric function type(WS="ws",sgw="sgw")', default='ws')
    parser.add_argument('-N', '--N_runs', help='Number of experiments',
                        type=int, default=1)
    parser.add_argument('-n', '--N_points', help='Number of points',
                        type=int, default=40)
    parser.add_argument('-p', '--permute', default=True, action='store_true')
    parser.add_argument('-f', '--noise_factor', default=0.0, type=float)
    parser.add_argument('-q', '--quick', default=False, action='store_true')
    parser.add_argument(
        '-o', '--output_name', help='output file name', default="loss_vs_time.png")
    args = parser.parse_args()

    # Experiment parameter
    N_runs = args.N_runs  # number of runs with current params

    # Type of datset
    if args.dataset == 's':
        data_func = get_spiral
    elif args.dataset == 'g':
        data_func = get_gaussian_point_cloud
    else:
        raise ValueError(
            f"Option {args.dataset} not recognized for the dataset.")

    # Type of metric function
    if args.metric_func == 'ws':
        metric_func = WS
        metric_name = 'WS'
    elif args.metric_func == 'sgw':
        metric_func = sgw_gpu_np
        metric_name = 'SGW'
    else:
        raise ValueError(
            f"Option {args.metric_func} not recognized for the metric function.")

    if args.quick:
        predictors = [IcpPredictor(max_iter=10, N_rots=10),
                      RisgwPredictor(max_iter=100)]
    else:
        predictors = [
            RisgwPredictor(nproj=500, max_iter=600, lr=0.001),
            RisgwPredictor(nproj=1000, max_iter=600, lr=0.001),
            RisgwPredictor(nproj=500, max_iter=1200, lr=0.001),
            IcpPredictor(max_iter=100, N_rots=50),
            IcpPredictor(max_iter=300, N_rots=50),
            IcpPredictor(max_iter=300, N_rots=100),
            IcpPredictor(max_iter=300, N_rots=150)]

    results = []
    for _ in trange(N_runs, desc="Run number:", leave=True):
        data_param = DataParam(metric_name=metric_name,
                               N_pts=args.N_points,
                               metric_func=metric_func,
                               data_func=data_func,
                               permute=args.permute,
                               noise_factor=args.noise_factor)

        results += evaluate_data_param(
            p=data_param, predictors=predictors)
        pk.dump(results, run_path.open('wb'))

    # Save the metrics
    fig, ax = plt.subplots(1, dpi=100, figsize=(15, 5))
    display_metric_vs_time_results(results, fig, ax)
    fig.savefig(f"results/{args.output_name}", bbox_inches='tight')


if __name__ == '__main__':
    main()
    # try:
    #    main()
    # except:
    #    extype, value, tb = sys.exc_info()
    #    traceback.print_exc()
    #    pdb.post_mortem()
